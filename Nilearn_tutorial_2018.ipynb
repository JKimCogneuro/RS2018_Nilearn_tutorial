{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Goals of today's tutorial\n",
    "==============\n",
    "\n",
    "This tutorial will cover the basics of using Nilearn to do machine learning with neuroimages. It is meant for people who do not necessarily have a background in programming. \n",
    "\n",
    "Because the tutorial is at a Resting State and Brain Connectivity, we will focus primarily on using resting state connectivity data.\n",
    "\n",
    "The specific goals of the day are broken into three sections:\n",
    "\n",
    "### Section 1: Nilearn 101\n",
    "* Use Python in a Jupyter notebook\n",
    "* Learn about the Nilearn API\n",
    "* Understand the difference between data stored in memory and data stored on disk\n",
    "* Load, plot and save 3D images\n",
    "\n",
    "### Section 2: Extracting features with Nilearn\n",
    "* Load and plot 4D images\n",
    "* Learn to extract rs-fmri data into connectivity features for machine learning\n",
    "\n",
    "### Section 3: Machine learning with Nilearn\n",
    "* Prepare a dataset for machine learning (i.e. train/test splits)\n",
    "* Build and validate a machine learning model\n",
    "* Use machine learning to predict unseen data\n",
    "* Interpret the features of a machine learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A few housekeeping notes:\n",
    "\n",
    "Lots of fantastic tutorials exists on the Nilearn website. Learning to use Nilearn almost certainly starts here!\n",
    "https://nilearn.github.io/index.html\n",
    "\n",
    "If things go poorly on your computer, join us online using Google Colab:\n",
    "*insert link here*\n",
    "\n",
    "Don't be afraid to ask questions! We have TAs to help out. And stop me if I'm going to fast!\n",
    "\n",
    "Don't forget, nobody ever writes error-free code!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Section 1: Nilearn 101\n",
    "============\n",
    "\n",
    "Here, we will learn how to run Python code in Jupyter notebooks, and learn a bit about how to use Nilearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's use this space to play a bit with Jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic nilearn example: manipulating and looking at data\n",
    "A simple example showing how to load an existing Nifti file and use\n",
    "basic nilearn functionalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use a Nifti file that is shipped with nilearn\n",
    "from nilearn.datasets import MNI152_FILE_PATH\n",
    "\n",
    "# Note that the variable MNI152_FILE_PATH is just a path to a Nifti file\n",
    "print('Path to MNI152 template: %r' % MNI152_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A first step: looking at our data\n",
    "Let's quickly plot this file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import plotting\n",
    "plotting.plot_img(MNI152_FILE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Try plotting one of your own files. In the above,\n",
    "MNI152_FILE_PATH is nothing more than a string with a path pointing to\n",
    "a nifti image. You can replace it with a string pointing to a file on\n",
    "your disk. Note that it should be a 3D volume, and not a 4D volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Try to plot your own file here:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple image manipulation: smoothing\n",
    "\n",
    "Let's use an image-smoothing function from nilearn:\n",
    ":func:`nilearn.image.smooth_img`\n",
    "\n",
    "Functions containing 'img' can take either a filename or an image as input.\n",
    "\n",
    "Here we give as inputs the image filename and the smoothing value in mm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import image\n",
    "smooth_anat_img = image.smooth_img(MNI152_FILE_PATH, fwhm=3)\n",
    "\n",
    "# While we are giving a file name as input, the function returns\n",
    "# an in-memory object:\n",
    "print(smooth_anat_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is an in-memory object. We can pass it to nilearn function, for instance to look at it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_img(smooth_anat_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could also pass it to the smoothing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "more_smooth_anat_img = image.smooth_img(smooth_anat_img, fwhm=3)\n",
    "plotting.plot_img(more_smooth_anat_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a moment to think about the relative merits of in-memory vs. on disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(smooth_anat_img.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = smooth_anat_img.shape\n",
    "print(x*y*z)\n",
    "# frames = \n",
    "# subjects = "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now imagine the size of a 4D image!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving results to a file\n",
    "\n",
    "We can save any in-memory object as follows:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "more_smooth_anat_img.to_filename('more_smooth_anat_img.nii.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Section 2: Extracting signals from a brain parcellation\n",
    "============================================\n",
    "\n",
    "Here we show how to extract signals from a brain parcellation and compute\n",
    "a correlation matrix.\n",
    "\n",
    "We also show the importance of defining good confounds signals: the\n",
    "first correlation matrix is computed after regressing out simple\n",
    "confounds signals: movement regressors, white matter and CSF signals, ...\n",
    "The second one is without any confounds: all regions are connected to\n",
    "each other.\n",
    "\n",
    "\n",
    "One reference that discusses the importance of confounds is `Varoquaux and\n",
    "Craddock, Learning and comparing functional connectomes across subjects,\n",
    "NeuroImage 2013\n",
    "<http://www.sciencedirect.com/science/article/pii/S1053811913003340>`_.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieve the atlas and the data\n",
    "--------------------------------\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're using rs-fmri data, it makes sense to use an atlas defined using rs-fmri data\n",
    "\n",
    "Let's use the MIST atlas, created here in Montreal using the BASC method. This atlas has multiple resolutions, for larger networks or finer-grained ROIs. Let's use a 64-ROI atlas to allow some detail, but to ultimately keep our connectivity matrices manageable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn import datasets\n",
    "\n",
    "parcellations = datasets.fetch_atlas_basc_multiscale_2015(version='sym')\n",
    "atlas_filename = parcellations.scale064\n",
    "\n",
    "\n",
    "print('Atlas ROIs are located in nifti image (4D) at: %s' %\n",
    "       atlas_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at that atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_roi(atlas_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, let's load an example 4D fmri time-series for one subject\n",
    "\n",
    "We have prepared some data especially for this tutorial. It is based on an open dataset of children and young adults. More details can be found here:\n",
    "\n",
    "https://openneuro.org/datasets/ds000228/versions/00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You will have to change this directory so it points to the RS2018_Nilearn_tutorial git folder\n",
    "# or the location of the files you downloaded\n",
    "git_path = '/Users/jakevogel/git/RS2018_Nilearn_tutorial/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have already downloaded the data, this next line should happen instantly. Otherwise, the data download will begin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, git_path)\n",
    "import download as dl\n",
    "\n",
    "# One subject of resting-state data\n",
    "data = dl.fetch_data(n_subjects=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmri_filenames = data.func[0]\n",
    "print('fmri timeseries are located in nifti image (4D) at: %s' %\n",
    "       fmri_filenames)  # 4D data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's have a look at that 4D resting-state image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(fmri_filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Uh oh! An error! What's the problem here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just a look at a single frame\n",
    "# First we load the image into memory\n",
    "myImg = image.load_img(fmri_filenames)\n",
    "print(myImg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's isolate the first frame\n",
    "first_frame = image.index_img(myImg, 0)\n",
    "print(first_frame.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now let's try to plot it\n",
    "plotting.plot_stat_map(first_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first frame can be a bit wonky. What about an average image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averaged_Img = image.mean_img(image.mean_img(myImg))\n",
    "averaged_Img.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And plot!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_stat_map(averaged_Img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract signals on a parcellation defined by labels\n",
    "Using the NiftiLabelsMasker\n",
    "\n",
    "So we've loaded our atlas and 4D data for a single subject. Let's practice extracting features!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# here is a short script to deal with the confounds in this particular dataset\n",
    "# don't worry about the details\n",
    "# but here is an example of how you can create your own function!\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def prepare_confounds(conf, key = 'R', transpose=True):\n",
    "    arrays = {}\n",
    "    f = h5py.File(conf)\n",
    "    for k, v in f.items():\n",
    "        arrays[k] = np.array(v)\n",
    "    \n",
    "    if transpose:\n",
    "        output = arrays[key].T\n",
    "    else:\n",
    "        output = arrays[key]\n",
    "    \n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.input_data import NiftiLabelsMasker\n",
    "masker = NiftiLabelsMasker(labels_img=atlas_filename, standardize=True, \n",
    "                           memory='nilearn_cache', verbose=5)\n",
    "\n",
    "# Here we go from nifti files to the signal time series in a numpy\n",
    "# array. Note how we give confounds to be regressed out during signal\n",
    "# extraction\n",
    "conf = prepare_confounds(data.confounds[0])\n",
    "time_series = masker.fit_transform(myImg, confounds=conf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what did we just create here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(time_series)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its a 168 (timeframes) x 64 (region) array!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are these \"confounds\" and how are they used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.confounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "DataFrame(conf).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute and display a correlation matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nilearn.connectome import ConnectivityMeasure\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')\n",
    "correlation_matrix = correlation_measure.fit_transform([time_series])[0]\n",
    "correlation_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the correlation matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# Mask the main diagonal for visualization:\n",
    "np.fill_diagonal(correlation_matrix, 0)\n",
    "\n",
    "# The labels we have start with the background (0), hence we skip the\n",
    "# first label\n",
    "plotting.plot_matrix(correlation_matrix, figure=(10, 8), \n",
    "                     labels=range(time_series.shape[-1]),\n",
    "                     vmax=0.8, vmin=-0.8, reorder=True)\n",
    "\n",
    "# matrices are ordered for block-like representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Same thing without confounds, to stress the importance of confounds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series = masker.fit_transform(myImg)\n",
    "# Note how we did not specify confounds above. This is bad!\n",
    "\n",
    "correlation_matrix = correlation_measure.fit_transform([time_series])[0]\n",
    "\n",
    "# Mask the main diagonal for visualization:\n",
    "np.fill_diagonal(correlation_matrix, 0)\n",
    "\n",
    "plotting.plot_matrix(correlation_matrix, figure=(10, 8), \n",
    "                     labels=range(time_series.shape[-1]),\n",
    "                    title='No confounds', reorder=True)\n",
    "\n",
    "plotting.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Section 3: Machine learning to predict age from rs-fmri\n",
    "\n",
    "We will integrate what we've learned in the previous sections to extract data from rs-fmri images, and use that data as features in a machine learning model\n",
    "\n",
    "The dataset consists of ~150 subjects, mostly young children and some young adults. We will use rs-fmri data to predict the age of the participants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data\n",
    "\n",
    "If you already have downloaded the data, the next commands should load it. If you have not, the data will begin downloading. This is a lot of data (more than 1GB) so maybe not good to download right now!\n",
    "\n",
    "You can still pop in after the \"extract features\" section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# if the data is already stored somewhere OR you want to store it somewhere, specify this here\n",
    "wdir = '/Users/jakevogel/Science/Nilearn_tutorial/ds000028/'\n",
    "\n",
    "# otherwise, comment the above and uncomment this\n",
    "# wdir = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now fetch the data\n",
    "data = dl.fetch_data(None,data_dir=wdir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many individual subjects do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data.func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Y and assess its distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load the phenotype data\n",
    "pheno = DataFrame(data.phenotypic)\n",
    "pheno.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is a column for age. Let's capture it in a variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_age = pheno['Age']\n",
    "y_age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maybe we should have a look at the distribution of our target variable\n",
    "\n",
    "The phenotype file comes with an AgeGroup variable. Let's take advantage of it to get a count on the age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pheno['AgeGroup'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems pretty skewed toward younger children. Let's plot the actual age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.hist(y_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very skewed indeed. Perhaps we will have better results by log-transforming age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_y_age = np.log(y_age)\n",
    "plt.hist(log_y_age)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's a bit better. Maybe we will want to use log-transformed age in our models?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we are going to use the same techniques we learned in section 2 to extract rs-fmri connectivity features from every subject.\n",
    "\n",
    "How are we going to do that? With a for loop.\n",
    "\n",
    "Don't worry, it's not as scary as it sounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here is a really simple for loop\n",
    "\n",
    "for i in range(10):\n",
    "    print('the number is', i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "container = []\n",
    "for i in range(10):\n",
    "    container.append(i)\n",
    "\n",
    "container"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets construct a more complicated loop to do what we want"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we do somethings we don't need to do in the loop. Let's re-iniate our masker and correlation_measure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# changing the verbosity of the masker\n",
    "masker = NiftiLabelsMasker(labels_img=atlas_filename, standardize=True, \n",
    "                           memory='nilearn_cache', verbose=0)\n",
    "correlation_measure = ConnectivityMeasure(kind='correlation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NOTE**: On a laptop, this might take quite a long time. It took 30 minutes on my mac. \n",
    "\n",
    "**Maybe don't run it right now!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from nilearn.connectome import sym_matrix_to_vec\n",
    "\n",
    "# all_features = [] # here is where we will put the data (a container)\n",
    "\n",
    "# for i,sub in enumerate(data.func):\n",
    "#     # convert confounds to readable format\n",
    "#     conf = prepare_confounds(data.confounds[i])\n",
    "#     # extract the timeseries from the ROIs in the atlas\n",
    "#     time_series = masker.fit_transform(sub, confounds=conf)\n",
    "#     # create a region x region correlation matrix\n",
    "#     correlation_matrix = correlation_measure.fit_transform([time_series])[0]\n",
    "#     # isolate non-redundant features\n",
    "#     upper_triang_corr = sym_matrix_to_vec(correlation_matrix, discard_diagonal=True)\n",
    "#     # add to our container\n",
    "#     all_features.append(upper_triang_corr)\n",
    "#     # keep track of status\n",
    "#     print('finished %s of %s'%(i+1,len(dataset.func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's save the data to disk\n",
    "# import os\n",
    "# outdir = os.getcwd()\n",
    "# np.savez_compressed(os.path.join(outdir,'BASC064_features'),a = all_features, b = full_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's do another loop to get data without confounds regressed to see if it makes a difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nilearn.connectome import sym_matrix_to_vec\n",
    "# no_conf = [] \n",
    "\n",
    "# for i,sub in enumerate(data.func):\n",
    "#     # notice we do not define or pass confounds here\n",
    "#     time_series = masker.fit_transform(sub)\n",
    "#     correlation_matrix = correlation_measure.fit_transform([time_series])[0]\n",
    "#     upper_triang_corr = sym_matrix_to_vec(correlation_matrix, discard_diagonal=True)\n",
    "#     no_conf.append(upper_triang_corr)\n",
    "#     print('finished %s of %s'%(i+1,len(data.func)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's save the data to disk\n",
    "# import os\n",
    "# outdir = os.getcwd()\n",
    "# np.savez_compressed(os.path.join(outdir,'BASC064_features_no_conf'),\n",
    "#                    a = no_conf, b = y_age)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case you do not want to run the full loop on your computer, you can load the output of the loop here!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# outdir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "feat_file = os.path.join(outdir,'BASC064_features.npz')\n",
    "X_features = np.load(feat_file)['a']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay so we've got our features. Why that shape?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nfeat = \n",
    "# nfeat * (nfeat-1) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize our feature matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(X_features, aspect='auto')\n",
    "plt.colorbar()\n",
    "plt.title('feature matrix')\n",
    "plt.xlabel('features')\n",
    "plt.ylabel('subjects')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data for machine learning\n",
    "\n",
    "Here, we will define a \"training sample\" where we can play around with our models. We will also set aside a \"test\" sample that we will not touch until the end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to be sure that our training and test sample are matched! We can do that with a \"stratified split\". Specifically, we will stratify by age group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "age_groups = pheno['AgeGroup']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the sample to training/test with a 60/40 ratio, and \n",
    "# stratify by age group, and also shuffle the data.\n",
    "\n",
    "X_train, X_test, y_train, y_test, ageGroup_train, ageGroup_test = train_test_split(\n",
    "                                                                X_features, \n",
    "                                                                y_age, \n",
    "                                                                age_groups,\n",
    "                                                                test_size = 0.4, \n",
    "                                                                shuffle = True,\n",
    "                                                                stratify = age_groups,\n",
    "                                                                random_state = 123\n",
    "                                                                                   )\n",
    "\n",
    "# print the size of our training and test groups\n",
    "print('training:', len(X_train),\n",
    "     'testing:', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's visualize the distributions to be sure they are matched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(y_train, label = 'train')\n",
    "plt.hist(y_test, label = 'test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run your first model!\n",
    "\n",
    "Machine learning can get pretty fancy pretty quickly. We'll start with a very standard linear model called a Support Vector Regressor (SVR). \n",
    "\n",
    "While this may seem unambitious, simple models can be very robust. And we don't have enough data to create more complex models.\n",
    "\n",
    "For more information, see this excellent resource:\n",
    "https://hal.inria.fr/hal-01824205"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our first model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "\n",
    "l_svr = SVR(kernel='linear') # define the model\n",
    "\n",
    "l_svr.fit(X_train, y_train) # fit the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well... that was easy. Let's see how well the model learned the data!\n",
    "\n",
    "We will judge our model on two criteria:\n",
    "* R2 = r-sqaure: the variance of the test data explained by the model\n",
    "* mae = mean absolute error: how off our measurements are in absolute units (years!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "y_pred = l_svr.predict(X_train) # predict the training data based on the model\n",
    "r2 = l_svr.score(X_train, y_train) # get the r2\n",
    "mae = mean_absolute_error(y_true = y_train, \n",
    "                          y_pred = y_pred) # get the mae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view our results and plot them all at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('r2 = %s, mae = %s'%(r2,mae))\n",
    "\n",
    "plt.scatter(y_train, y_pred)\n",
    "plt.title('Predicted vs Observed')\n",
    "plt.xlabel('Predicted Age')\n",
    "plt.ylabel('True age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HOLY COW! Machine learning is amazing!!! Almost a perfect fit!\n",
    "\n",
    "...which means there's something wrong. What's the problem here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_predict, cross_val_score\n",
    "\n",
    "# predict\n",
    "y_pred = cross_val_predict(l_svr, X_train, y_train, \n",
    "                           groups=ageGroup_train, cv=10)\n",
    "# scores\n",
    "r2 = cross_val_score(l_svr, X_train, y_train, \n",
    "                     groups=ageGroup_train, cv=10)\n",
    "mae_score = cross_val_score(l_svr, X_train, y_train, \n",
    "                            groups=ageGroup_train, cv=10,\n",
    "                           scoring = 'neg_mean_absolute_error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the accuracy of the predictions for each fold of the cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the overall accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "\n",
    "overall_r2 = r2_score(y_pred = y_pred, y_true = y_train)\n",
    "overall_mae = mean_absolute_error(y_pred = y_pred, y_true = y_train)\n",
    "print('r2 = %s, mae = %s'%(overall_r2,overall_mae))\n",
    "\n",
    "plt.scatter(y_pred, y_train)\n",
    "plt.title('Predicted vs Observed')\n",
    "plt.xlabel('Predicted Age')\n",
    "plt.ylabel('True age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not too bad at all! But what are some things you notice about our results?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweak your model\n",
    "\n",
    "It's very important to learn when and where its appropriate to \"tweak\" your model.\n",
    "\n",
    "Since we have done all of the previous analysis in our training data, it's find to try different models. But we **absolutely cannot** \"test\" it on our left out data. If we do, we are in great danger of overfitting.\n",
    "\n",
    "We could try other models, or tweak hyperparameters, but we are probably not power sufficiently to do so, and would once again risk overfitting.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But we can perhaps look at the performance of the model on log-transformed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "log_y_train = np.log(y_train) # log-transform target data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repeat the steps above to re-fit the model \n",
    "# and assess its performance\n",
    "\n",
    "# don't forget to switch y_train to log_y_train\n",
    "y_pred = cross_val_predict(l_svr, X_train, log_y_train, groups=ageGroup_train, cv=10)\n",
    "r2 = cross_val_score(l_svr, X_train, log_y_train, groups=ageGroup_train, cv=10)\n",
    "mae_score = cross_val_score(l_svr, X_train, log_y_train, groups=ageGroup_train, cv=10,\n",
    "                           scoring = 'neg_mean_absolute_error')\n",
    "\n",
    "# don't forget to switch y_train to log_y_train\n",
    "overall_r2 = r2_score(y_pred = y_pred, y_true = log_y_train)\n",
    "overall_mae = mean_absolute_error(y_pred = y_pred, y_true = log_y_train)\n",
    "print('r2 = %s, mae = %s'%(overall_r2,overall_mae))\n",
    "\n",
    "plt.scatter(y_pred, log_y_train)\n",
    "plt.title('Predicted vs Observed')\n",
    "plt.xlabel('Predicted Age')\n",
    "plt.ylabel('True age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think about the results of this model compared to the non-transformed model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** Try fitting a new SVR model and tweak one of the many parameters. Run cross-validation and see how well it goes. Make a new cell and type SVR? to see the possible hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# new_model = SVR() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can our model predict age in completely un-seen data?\n",
    "Now that we're fit a model we think has possibly learned how to decode age based on rs-fmri signal, let's put it to the test. We will train our model on all of the training data, and try to predict the age of the subjects we left out at the beginning of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because we performed a transformation on our training data, we will need to transform our testing data using the *same information!* \n",
    "\n",
    "For that, we will need to create a transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# transform testing set based on training distribution...\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "\n",
    "transformer = FunctionTransformer(np.log).fit(y_train.values.reshape(-1,1))\n",
    "log_y_test = transformer.transform(y_test.values.reshape(-1,1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# how did we do?\n",
    "plt.hist(log_y_train, label = 'train')\n",
    "plt.hist(log_y_test, label = 'test')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now for the moment of truth! \n",
    "\n",
    "No cross-validation needed here. We simply fit the model with the training data and use it to predict the testing data\n",
    "\n",
    "I'm so nervous. Let's just do it all in one cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_svr.fit(X_train, log_y_train) # fit to training data\n",
    "y_pred = l_svr.predict(X_test) # predict age using testing data\n",
    "r2 = l_svr.score(X_test, log_y_test) # get r2 score\n",
    "mae = mean_absolute_error(y_pred=y_pred, y_true=log_y_test) # get mae\n",
    "\n",
    "# print results\n",
    "print('r2 = %s, mae = %s'%(r2,mae))\n",
    "\n",
    "# plot results\n",
    "plt.scatter(y_pred, log_y_test)\n",
    "plt.title('Predicted vs actual age')\n",
    "plt.xlabel('Predicted age')\n",
    "plt.ylabel('True age')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Wow!!*** Congratulations. You just trained a machine learning model that used real rs-fmri data to predict the age of real humans.\n",
    "\n",
    "It seems like something in this data does seem to be systematically related to age ... but what?  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting model feature importances\n",
    "Interpreting the feature importances of a machine learning model is a real can of worms. This is an area of active research. Unfortunately, it's hard to trust the feature importance of some models. \n",
    "\n",
    "You can find a whole tutorial on this subject here:\n",
    "http://gael-varoquaux.info/interpreting_ml_tuto/index.html\n",
    "\n",
    "For now, we'll just eschew better judgement and take a look at our feature importances"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can access the feature importances (weights) used my the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l_svr.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lets plot these weights to see their distribution better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(range(l_svr.coef_.shape[-1]),l_svr.coef_[0])\n",
    "plt.title('feature importances')\n",
    "plt.xlabel('feature')\n",
    "plt.ylabel('weight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or perhaps it will be easier to visualize this information as a matrix similar to the one we started with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A little matrix manipulation to wrangle our features \n",
    "\n",
    "n_roi = 64\n",
    "feat_exp_matrix = np.zeros((n_roi,n_roi))\n",
    "upper_tri_coord = np.triu_indices_from(feat_exp_matrix,1)\n",
    "feat_exp_matrix[upper_tri_coord] = l_svr.coef_[-1]\n",
    "feat_exp_matrix.T[upper_tri_coord] = feat_exp_matrix[upper_tri_coord]\n",
    "\n",
    "# and plot a heatmap\n",
    "plotting.plot_matrix(feat_exp_matrix, figure=(10, 8), \n",
    "                     labels=range(n_roi),\n",
    "                     reorder=False,\n",
    "                    tri='lower')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if we can throw those features onto an actual brain.\n",
    "\n",
    "First, we'll need to gather the coordinates of each ROI of our atlas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from nibabel import Nifti1Image\n",
    "\n",
    "# def lazy_get_coords(atlas_filename):\n",
    "#     coords = []\n",
    "#     img = image.load_img(atlas_filename)\n",
    "#     atlas_data = img.get_data()\n",
    "#     aff = img.affine\n",
    "#     values = np.unique(atlas_data)[1:]\n",
    "#     for i in values:\n",
    "#         roi = np.zeros_like(atlas_data)\n",
    "#         roi[atlas_data==i] = 1.0\n",
    "#         coords.append(plotting.find_xyz_cut_coords(Nifti1Image(roi,aff)))\n",
    "    \n",
    "#     return coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# atlas_coordinates = lazy_get_coords(atlas_filename)\n",
    "# np.savez_compressed(os.path.join(outdir,'BASC064_coordinates'),\n",
    "#                    a = atlas_coordinates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coords = np.load(os.path.join(outdir,'BASC064_coordinates.npz'))['a']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now we can use our feature matrix and the wonders of nilearn to create a connectome map where each node is an ROI, and each connection is weighted by the importance of the feature to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotting.plot_connectome(feat_exp_matrix, coords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix: Goodies and extra points"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise:** There are is a clear distinction between adults in children in this dataset. See if you can train a classifier (for example, SVC) to predict which subjects are adults and which are children. \n",
    "\n",
    "Return your overall accuracy, but also the positive predictive value (precisions score). You can tweak your model, but remember, don't burn your test data or your results don't count! Also remember you don't have many subjects to work with in the first place. \n",
    "\n",
    "For a bonus, plot the feature importances, and correlate the feature importances with those from our linear model predicting age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# from sklearn.svm import SVC\n",
    "# y_class_train = [1 if x==b'Adult' else 0 for x in ageGroup_train]\n",
    "# y_class_test = [1 if x==b'Adult' else 0 for x in ageGroup_test]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
